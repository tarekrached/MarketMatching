# Market Matching and Causal Impact Inference
If you ever spent time in the field of marketing analytics, chances are that you have analyzed the causal impact of various events (interventions) using time series techniques. An event in this context could be a new TV or radio campaign, a major PR event, or some external event such as a new local competitor. These are all events where we cannot track the impact at the individual customer level and hence have to analyze the impact from a bird's eye view – i.e., we have to analyze aggregated time series data, typically at the market level (e.g., DMA, state, etc.). Data science may be changing, but this is a use-case that has been around forever and is still very relevant.  

An intervention analysis usually involves two steps:

1. Find matching *control* markets for the *test* market where the event took place, using time series matching based on data prior to the event (pre period).
2. Analyze the causal impact of the event by comparing the observed data for the test and control markets following the event (post period), factoring in differences prior to the event. 

## A Traditional Approach
For step 1, the most straight-forward approach to this would be to use the Euclidian distance to find the best matching control market. However, this approach implicitly over-penalizes instances where the markets are shifted. Although it is preferable for test and control markets to be aligned consistently, occasional historical shifts should not eliminate good control market candidates. 

For step 2, the traditional approach is a "diff in diff" model where the difference between the test market and the best matching control market is modeled. However, this assumes  are i.i.d. and that the differences between the test and control markets is constant – assumptions that rarely hold true for time series data.

## A More Flexible and Robust Approach
A better approach is to use *dynamic time warping* for the time series matching step. This technique finds the distance along the *warping curve* – instead of the raw data – where the warping curve represents the best alignment between two time series within some user-defined constraint (time window). 

For the intervention analysis, `CausalImpact` package created by Kay Brodersen at Google provides an approach that is more flexible than "diff in diff"  (see [1]). The `CausalImpact` package constructs a synthetic baseline for the post-intervention period based on a structural time series model with components on based *multiple* matching control markets. 

The `MarketMatching` implements this workflow in the following way:

1. Find the best control markets for each market in the dataset using dynamic time warping. The user can define how many matches should be retained. Also, the user can choose whether the market screening should rather be based on correlation or some blend of dynamic time warping and correlation.

2. For a give test market, pass the markets selected in step 1 to the `CausalImpact` package to do the post intervention analysis. `CausalImpact` creates a synthetic control by fitting a Bayesian structural time series model and producing a counterfactual prediction for the post period. The difference between the synthetic control and the test market (for the post-intervention period) are then comppared to the posterior interval. The number of markets used to create the synthetic control is usually a smaller number than the number of markets passed from step 1 (as selected by spike-and-slab approach).

### Notes on the Workflow
The dynamic time warping step is not a strictly necessary step to execute this workflow; we can select markets directly while building the time series model during step 2. In fact, the `CausalImpact` package selects the most predictive markets for the structural time series model using spike-and-slab priors. This, we could jump directly to the `CausalImpact` package without pre-screening the markets. 

However, when dealing with a large set of candidate control markets it often turns out to be prudent to trim the list in advance, as opposed to relying solely on the variable selection process. In other words, pre-screening the markets is akin to wearing belt and suspenders; you're much less likely to get a strange and indefensible model. Moreover, basing the synthetic control on markets that have small *distances* to the test market boosts the face-validity of the analysis as these control markets will be similar in "size" and hence be recognized as viable controls by your business partners.  Ultimately, however, this is a matter of preference and the good news is that the `MarketMatching` package allows users to decide how many control markets should pass through step 1.

Last but not least, one can argue that the initial screening should be correlation-based and that distances are irrelevant – i.e., the "sizes" of the control markets do not matter as long as they are predictive. Again, this is a matter of preferences and `MarketMatching` allows the user to control whether the ranking is correlation-based or distance-based.   

# About MarketMatching Package

The `MarketMatching` package implements the workflow described above by essentially providing a "wrapper" for the `dtw` and `CausalImpact` packages. Hence, the package does *not* provide any functionality that cannot be found in these packages, but rather simplifies the workflow of using `dtw` and `CausalImpact` together and provides charts and data that are easy to manipulate. R packages are simply a great way of implementing and documenting workflows.

## Summary of features:

* Minimal inputs required. The only strictly necessary inputs are the name of the test market (for inference) and the dates of the pre-period and post-period.
* Provides a data.frame with the best $K$ matches for all markets in the input dataset. $K$ can be defined by the user.
* Outputs all inference results as objects with intuitive names (e.g., "AbsoluteEffect" and "RelativeEffect").
* Calculates MAPE and Durbin-Watson for the pre-period. Shows how these statistics change when you alter the prior standard error of the local level term.
* Plots and outputs the actual data for the markets selected during the initial market matching.
* Plots and outputs actual versus predicted values.
* Plots the final local level term.
* Shows the average estimated coefficients for all the markets used in the linear regression component of the structural time series model.
* Allows the user to choose how many markets are sent to the slab-and-prior model.
* All plots are done in `ggplot2` and can easily be manipulated.

# How to Install
```{r, eval=FALSE}
library(devtools)
install_github("google/CausalImpact")
install_github("klarsen1/MarketMatching", "klarsen1", build_vignettes=TRUE)
```

# To Learn More About this Package
The vignette shows examples and provides a brief overview of the theory. Check out the vignette by using the following command in `R`:

```{r, eval=FALSE}
browseVignettes("MarketMatching")
```

# Example
```{r, echo = TRUE, message=FALSE, eval=FALSE}
library(MarketMatching)
##-----------------------------------------------------------------------
## Find the best matches (default is 5) for each airport time series
##-----------------------------------------------------------------------
library(MarketMatching)
data(weather, package="MarketMatching")
mm <- best_matches(data=weather, 
                   id_variable="Area", 
                   date_variable="Date", 
                   matching_variable="Mean_TemperatureF", 
                   parallel=TRUE,
                   start_match_period="2014-01-01",
                   end_match_period="2014-10-01")
##-----------------------------------------------------------------------
## Analyze causal impact of a made-up weather intervention in Copenhagen
## Since this is weather data this is a meaningless example and we should 
## expect no causal impact. This is just to demo the function.
##-----------------------------------------------------------------------
library(CausalImpact)
results <- MarketMatching::inference(matched_markets = mm, 
                                    test_market = "CPH", 
                                    end_post_period = "2015-10-01")
``` 

A view of the best matches data.frame generated by the best_matches() function:
```{r, echo = TRUE, message=FALSE, eval=FALSE}
knitr::kable(head(mm$BestMatches))
```

Plotting the absolute impact
```{r, echo = TRUE, message=FALSE, eval=FALSE}
results$PlotAbsoluteEffect
```

Plot actual observations for test market (CPH) versus the expectation
```{r, echo = TRUE, message=FALSE, eval=FALSE}
results$PlotActualVersusExpected
```

Store actual versus predicted in a data.frame
```{r, echo = TRUE, message=FALSE, eval=FALSE}
pred <- results$Predictions
knitr::kable(head(pred))
```

PLot the actual data for the test and control markets
```{r, echo = TRUE, message=FALSE, eval=FALSE}
results$PlotActuals
```

Check DW, MAPE and largest market coefficient for different values of the local level SE
```{r, echo = TRUE, message=FALSE, eval=FALSE}
results$PlotPriorLevelSdAnalysis
```

Store the coefficients in a data.frame
```{r, echo = TRUE, message=FALSE, eval=FALSE}
coeff <- results$Coefficients
knitr::kable(head(coeff))
```

# References
[1] CausalImpact version 1.0.3, Brodersen et al., Annals of Applied Statistics (2015). http://google.github.io/CausalImpact/

[2] Vignette for the `dtw` package: https://cran.r-project.org/web/packages/dtw/vignettes/dtw.pdf.

[3] Predicting the Present with Bayesian Structural Time Series, Steven L. Scott and Hal Varian, http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-with-bsts.pdf.